{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27447179",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MeyerBender/data_analysis_workshop/blob/main/notebooks/02_exploratory_data_analysis.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f08290-b58e-4e95-b785-e37c9dadf4f9",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "In this notebook, we will go over how to approach a dataset. The concepts you learn here are applicable to all kinds of tabular data. The basic workflow is as follows:\n",
    "\n",
    "1. Read, clean and preprocess the data.\n",
    "2. Perform some basic statistical analysis, e. g. looking at the mean, median and standard deviation of a variable.\n",
    "3. Visualization and data exploration.\n",
    "\n",
    "The data set we will be looking at contains information about breast cancer patients. Features describe the shape of the lobe in each patient. More information can be found at [Kaggle](https://www.kaggle.com/datasets/yasserh/breast-cancer-dataset?resource=download). Note that the original data set was slightly altered to illustrate certain concepts of data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1e1831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the umap package\n",
    "! pip install --quiet umap-learn\n",
    "\n",
    "# download the data\n",
    "# if you have already run this cell once, there is no need to run it again\n",
    "! wget https://www.huber.embl.de/users/matthias/breast-cancer-modified.csv /content/breast-cancer-modified.csv\n",
    "data_dir = '/content'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f69cc0-4f26-4b99-bb51-5714d147d969",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Setup\n",
    "Here, we quickly import all of the required packages, so that we can use their methods later. Importing using short aliases (e. g. `import numpy as np`) is common practice, since it saves us some typing down the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a64f9be-c2e6-4517-9e07-d5bc390fc54b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the os module facilitates working with files and directories\n",
    "import os\n",
    "# numpy contains methods for basic array manipulations\n",
    "import numpy as np\n",
    "# pandas provides data frames, which are very useful when dealing with tabular data\n",
    "import pandas as pd\n",
    "# these two libraries are for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17f1043-64da-473f-8826-5c6c2eb0174c",
   "metadata": {},
   "source": [
    "# Data Reading, Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cedf6f-1c42-4a0b-83bd-60ad81fe8a8e",
   "metadata": {},
   "source": [
    "First, we need to read in the data. For this, we can use the function `pd.read_csv()`. We store the result in a variable we call `df` (for data frame)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf05f08-0490-4b97-a1f2-1d6f5c49b589",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reading in the data from a csv file\n",
    "# use the first column as index (set index_col=0)\n",
    "df = pd.read_csv(os.path.join(data_dir, 'breast-cancer-modified.csv'), index_col=0)\n",
    "# printing the shape and first couple of rows of the data frame\n",
    "print(df.shape)\n",
    "# note that you can leave out the print() if you are in the last line \n",
    "# of a cell in a jupyter notebook\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e5a5bd-e1fb-48b0-a4ab-1e7646493365",
   "metadata": {},
   "source": [
    "Next, we want to see how good our data is. Two common issues are missing data and duplicates (e. g. if two doctors entered information about a patient simultaneously). Check if there are any in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31af451-449d-415a-ad93-2676a37435eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# checking for duplicate rows\n",
    "# hint: after getting duplicated values, the np.any() method might help\n",
    "\n",
    "# === your code here ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f5dd26-15fd-4ab6-b4e3-e7e137a67d28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# removing duplicate rows\n",
    "\n",
    "# === your code here ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addbbde9-f543-4312-aaa1-2bef9a2e9772",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# checking the shape of the data frame again. What changed?\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8f6df4-b6e8-4098-9aeb-873532ca947b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# checking for missing values\n",
    "\n",
    "# === your code here ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189c9b09-f080-4cf3-87da-10d811fa7d4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# removing columns that contain missing values\n",
    "\n",
    "# === your code here ===\n",
    "\n",
    "# checking the shape of the data frame\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4e0c79-ff66-4b82-b5d9-614147c3f0e1",
   "metadata": {},
   "source": [
    "Here, we simply chose to remove all rows that contained NA values. You could also consider removing the features (columns) that contain NAs, or try to interpolate the missing values from other variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ec7924-af5e-46a6-96c1-03bc4a4946fe",
   "metadata": {},
   "source": [
    "# Statistical Analysis\n",
    "Now that our data is nice and cleaned up, we can start looking at what we have. Common things to look at for numerical data are mean, median, and standard deviation. When looking at categorical data, you could look at the unique values and how many of each there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e7f2e9-3cb0-450e-9cde-7bad8b190190",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# start by looking at the features we have\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71b8d11-ea50-44dd-9336-276fd01d7ae4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# choose a numerical feature of interest\n",
    "# compute the mean, median and standard deviation using numpy or pandas\n",
    "\n",
    "# === your code here ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2690fee-e333-4a11-9aac-6cfed9c4e156",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check if there are any categorical variables in the data\n",
    "# if so, check which values are present, and count how many of each value there are\n",
    "\n",
    "# === your code here ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6465385-484b-4fcd-8a79-c37ea7c4c714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cool, now we can already investigate individual features\n",
    "# but is there a pandas method that can compute a bunch of statistics at once?\n",
    "# see if you can find it\n",
    "\n",
    "# === your code here ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad06b48-99a0-4d21-9c81-c6c830424ca6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Basic Visualization\n",
    "\n",
    "Looking at numbers is cool and all, but it is hard to find any patterns in there. This is where visualization comes in. Here, we will look at a bunch of visualizations that help you to get a better feel for your data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a917d8d-97fd-464e-8fb3-b5d7d76d3456",
   "metadata": {},
   "source": [
    "## Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bd8bc7-d8ca-48b5-9d9d-109a6be393a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# while things like median and standard deviation give you a rough feel for the data,\n",
    "# it is usually better to just look at the underlying distribution\n",
    "# choose one of the numerical features and plot its distribution using pyplot \n",
    "# remember we imported this as plt, so you can do something like plt.hist()\n",
    "\n",
    "# === your code here ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23a2e12-61ad-4a06-a0fe-3e2f0a309f4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# what happens if you increase/decrease the number of bins in the histogram?\n",
    "\n",
    "# === your code here ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d79b387-e613-482a-9b81-14131e4af61e",
   "metadata": {},
   "source": [
    "## Scatter Plots\n",
    "Looking at one feature is cool, but what is better than one? Exactly, two!\n",
    "Use seaborn to create a scatter plot that plots one numerical feature against a different one. Color the points according to if the cancer is benign or malignant. \n",
    "Do you see any patterns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b1e2f0-4372-4f43-b4b6-7d6d8de890c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a scatter plot that plots two features against one another\n",
    "# use sns.scatterplot() for this\n",
    "# HINT: in seaborn, you can set the color with hue=\"column_of_interest\"\n",
    "\n",
    "# === your code here ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbd153c-29de-46aa-b16d-946a8d2a69de",
   "metadata": {},
   "source": [
    "## Correlations and Heatmaps\n",
    "\n",
    "In real life datasets, features are often correlated with one another. We can compute pairwise correlations between features to assess if this is the case.\n",
    "\n",
    "Compute the correlation between the two features you chose for the previous plot. Are they correlated with one another? How much?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e22a641-df65-4edb-9a83-0f9e82cd4a02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compute the correlation between the two features\n",
    "# you can use numpy, pandas, or even try to import scipy to also obtain a p-value\n",
    "\n",
    "# === your code here ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a86435-470a-4a86-8e5e-eb80c1f6e5e8",
   "metadata": {},
   "source": [
    "Okay, so we know how to compute pairwise correlations. Can you do this for all possible combinations of features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027e45bc-9f34-46c6-8495-beadbea48a7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compute the pairwise correlation matrix for all features in the data frame\n",
    "# what happens to the diagnosis column?\n",
    "\n",
    "# === your code here ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2489b2eb-48c1-4a13-b43c-9f168414769e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot the resulting correlation matrix as a heatmap using seaborn\n",
    "# make sure that your color scale makes sense\n",
    "\n",
    "# === your code here ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d561ba-7bb6-4bc5-8628-05c23f0cf5fa",
   "metadata": {},
   "source": [
    "# Visualization of High Dimensional Data\n",
    "\n",
    "30 features are quite a lot, and our minds are certainly not wired for visualizing data points in 30-dimensional space. Instead, we need to come up with some workarounds. Here, we can look at how to use heatmaps to visualize such datasets. We will also play around with some dimensionality reduction methods to enable us to visualize our high-dimensional data in 2D space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47095e1-d058-4931-af58-cb21c7749182",
   "metadata": {},
   "source": [
    "## Visualization with Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb373268-6b94-4bd9-a5d8-1131817cd5bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a heatmap showing all features for all patients\n",
    "# you might need to subselect from the data frame so that you only use numerical features in the heatmap\n",
    "\n",
    "# === your code here ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23ef152-f890-4480-a097-5250ad6b2b90",
   "metadata": {},
   "source": [
    "## Normalization and Standardization\n",
    "\n",
    "I'm sure the previous heatmap looks awesome, but you might have spotted a small problem: the features are all in very different ranges. For example, the area of a lobe is obviously going to be substantially bigger than its radius.\n",
    "\n",
    "To make it easier to see what is going on, we can try to scale our data. There are several ways we could do this. \n",
    "\n",
    "For example, we could try to scale every feature between 0 and 1. This is commonly referred to as **normalization**. \n",
    "\n",
    "Alternatively, you could alter the data so that the mean of each feature is at 0, and the standard deviation is one. This is called **standardization** or **z-score normalization**.\n",
    "\n",
    "You might already be familiar with standardization. When performing Principal Component Analysis (PCA), you first have to center and scale your data. That's basically what we do: we center the data (so that the mean is zero) and then scale it (so that each feature has a standard deviation of one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d745b28-803d-4b76-838c-59313903bd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform standardization on the data frame\n",
    "# store the standardized data frame in a variable called df_standardized\n",
    "# for standardization, you can use the StandardScaler from sklearn\n",
    "# however, you will need to import it first\n",
    "# HINT: you cannot standardize a categorical feature, so make sure to remove it before applying the StandardScaler\n",
    "# you can add it back to the standardized data frame afterwards if you need it\n",
    "\n",
    "# === your code here ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30143955-e8f7-4523-8bb8-9775c3c724b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now try creating the heatmap from before again. \n",
    "# do you see the difference?\n",
    "# pro tip: always make sure that your color scale makes sense (e. g. 0 is at the center)\n",
    "# you can use center=0 in seaborns heatmap function to achieve this\n",
    "\n",
    "# === your code here ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d5fe28-a277-456b-96bc-16f7e1711a0f",
   "metadata": {},
   "source": [
    "## Clustermaps\n",
    "\n",
    "Even with standardization, it is still hard to see what is going on. There might be some patterns in there, but we cannot see them due to the sheer amount of data.\n",
    "\n",
    "One approach to reveal those hidden patterns is **clustering**. Clustering is a form of unsupervised learning. This means that it tries to find patterns in the data without any input from us. In essence, it tries to group similar things together and keep dissimilar things apart.\n",
    "\n",
    "This is perfect for us! We can try to cluster our patients to see if clustering finds some patterns that potentially relate to if a cancer is benign or malignant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b84532c-b40f-4a9e-95ab-64696b8c328f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a seaborn clustermap of the standardized data frame\n",
    "\n",
    "# === your code here ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83496b3b-6f04-4fba-a161-5bc1fd2eebc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# try if you can add information about the diagnosis into the clustermap\n",
    "# HINT: the row_colors argument might help\n",
    "\n",
    "# === your code here ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19abff4a-1678-46af-9653-692591df63ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# just for fun, try to see what happens if you now set col_cluster and row_cluster to False\n",
    "# compare the clustermap to the previous one\n",
    "# did clustering help to reveal patterns?\n",
    "\n",
    "# === your code here ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c02047d-f35f-4e57-8c83-48e5cc3aee44",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "Heatmaps are awesome, and they allow us to show a lot of information at once. However, they can be restrictive when looking at large amounts of data. Sometimes, it is better to use dimensionality reduction techniques.\n",
    "\n",
    "PCA essentially takes our 30-dimensional data and projects it onto a 2-dimensional space while retaining the most variance. This enables us to plot each patient as a point in 2D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3206955-ca55-44fe-864d-a8282a57db3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# importing PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# apply PCA to our standardized (this is important! always center and scale before applying PCA)\n",
    "# data frame. Store the results in a new data frame called pca_df. Only keep the first two principal components.\n",
    "\n",
    "# === your code here ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9f3a64-e812-4e3f-bce1-efe73c05e3e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add the diagnosis as a new column to the pca_df.\n",
    "# create a scatterplot using seaborn with PC1 on the x axis, PC2 on the y axis, and \n",
    "# the diagnosis as the color\n",
    "\n",
    "# === your code here ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985dcaf8-1f41-4ed2-a68e-d4808060ef79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# you can access the explained variance ratio (EV) of each principal component\n",
    "# (if you don't know how, I recommend asking ChatGPT. Remember to provide your code to get better results.)\n",
    "# access the explained variance ratio and change the axis labels \n",
    "# for example, instead of PC1, the x axis should say PC1 (EV=21%)\n",
    "\n",
    "# === your code here ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db1a8c9-7112-4c05-b43c-e3ec1d9687cc",
   "metadata": {},
   "source": [
    "## UMAP\n",
    "\n",
    "PCA is cool and interpretable, but there are more advanced dimensionality reduction methods. One of these is UMAP. The details of this go beyond the scope of this practical, for you it is enough to know that you can use UMAP to project from high-dimensional spaces into 2D space. One important note: UMAP does not preserve distances faithfully, so **never cluster on UMAP space!** Only use UMAP for visualization purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0854f01-bd22-4943-ba0b-dde42dd86ed1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "# run umap to project our raw data onto 2D space\n",
    "# store the result in a data frame called umap_df_raw\n",
    "# add the diagnosis column into the resulting data frame\n",
    "\n",
    "# === your code here ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2b0ac9-2f9d-4c68-943f-0be8b66aab05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot a scatter plot with UMAP1 and UMAP2 on the x and y axis and the diagnosis as hue\n",
    "\n",
    "# === your code here ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98021df3-61b9-4bb4-a26d-187b480ac8c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now try applying UMAP on the standardized data\n",
    "# do you see any differences? Do you think standardization is necessary for UMAP?\n",
    "# check out https://github.com/lmcinnes/umap/issues/66 for more details on this\n",
    "\n",
    "# === your code here ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eab1b4e-c410-416c-ac0b-5d26a9f49a35",
   "metadata": {},
   "source": [
    "## Catplots\n",
    "One last option of looking at high dimensional data is to look at feature distributions. You already looked at histograms for single features before, but what if we could plot a bunch of feature distributions in a single plot? This is where catplots (categorical plots) come in. You are probably familiar with bar- or boxplots, but there are also other types, such as violinplots or swarmplots. You can find more information on these types in the [seaborn documentation](https://seaborn.pydata.org/generated/seaborn.catplot.html).\n",
    "\n",
    "The reason why we are only now looking at these plots is that creating them requires some reshaping of the data frame. We will hence use this section to look into pandas a bit more, and then explore some ways of plotting the data.\n",
    "\n",
    "Let's start by reshaping the data frame. Our goal is to go from a data frame with the columns [feature1, feature2, feature2, ...] to a data frame with columns [feature, value]. We can do this by using the `pd.melt()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f43efa5-bab7-4c8c-a081-1e5c35a47696",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# select only numerical features from the data frame\n",
    "# melt the data frame to get a new data frame containing one column for the features and one for the values\n",
    "\n",
    "# === your code here ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fabadc-e18f-48d7-8625-c6082f67b09a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a barplot of all the features using seaborn's catplot method\n",
    "# try using the \"aspect\" parameter to change the size of the figure\n",
    "\n",
    "# === your code here ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e113cd2-ce4d-45b9-a72b-ce5cc7120dde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now do the same thing, but with a boxplot instead of a barplot\n",
    "# do you see how the boxplot could be misleading, and how showing more of the data gives us a better understanding?\n",
    "\n",
    "# === your code here ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca597bb-7276-4ef0-98e3-ee1b39326ccb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# select three features from the data frame\n",
    "# create a swarmplot for the resulting subselected data frame\n",
    "\n",
    "# === your code here ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9142b00f-1a11-48b8-83bd-12718358622c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Advanced Plotting\n",
    "\n",
    "Amazing work! Now you are set to begin your journey as a data scientist!\n",
    "Just before wrapping up, here are some quick tips on how to create nicer plots. It is often not too much of an effort to prettify your plots, but it makes it look substantially more professional. It can also help you get a better grasp of your data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a15f5d-fd00-493b-96a7-9cd6ac952f14",
   "metadata": {},
   "source": [
    "## Create Meaningful Color Scales\n",
    "\n",
    "When plotting a heatmap, you typically have two types of color scales: **continuous** or **divergent**. Continuous color scales go from one color to another, for example white to red. Divergent color schemes typically consist of three colors, e. g. blue, black and red.\n",
    "When your color scale has a meaningful center point (e. g. because you centered your data), use a divergent scale. Otherwise, a continuous one might be more appropriate.\n",
    "\n",
    "You can find possible color palettes [here](https://seaborn.pydata.org/tutorial/color_palettes.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaec7e6-b8c3-49d1-9c18-78baa41e3518",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# example: in this case, a continuous color scale makes more sense, because our data goes from 0 to some number\n",
    "plt.figure(figsize=(15, 5))\n",
    "sns.heatmap(df.iloc[:, 1:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e43cd5c-33b2-40d7-b409-2d049e7e1906",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# example: in this case, a divergent color scale is more sensible,\n",
    "# because we go from negative over neutral (0) to positive\n",
    "# we can even set a custom palette\n",
    "palette = sns.diverging_palette(220, 20, 1000, as_cmap=True)\n",
    "plt.figure(figsize=(15, 5))\n",
    "sns.heatmap(df_standardized.iloc[:, :-1], center=0, cmap=palette)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82706cbf-28df-45f1-adb6-2530a4e7381b",
   "metadata": {},
   "source": [
    "## Nicer Scatterplots in Seaborn\n",
    "\n",
    "While seaborn plots already look decent out of the box, there are some things you can tweak to make them look even better.\n",
    "\n",
    "For example, removing the top and right axis and playing around with colors and font sizes can make a massive difference. You can also see what changing the size of your points does. Here is the same UMAP plot from before, but prettified.\n",
    "\n",
    "If you want to create a custom color scheme, [Coolors](https://coolors.co) has some good inspiration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee84c62e-9400-4ef9-bd8f-ed8421d35f16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set custom parameters for seaborn\n",
    "# if you put this at the top of your notebook, all of your plots will inherit these settings\n",
    "custom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False, \"figure.dpi\": 150}\n",
    "sns.set_theme(style=\"ticks\", rc=custom_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e439b8-b940-4272-9b3d-11ab69bdbcf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# custom colors\n",
    "# this is simply a dictionary that maps our possible diagnoses to the hex code of a color\n",
    "color_mapping = {\"M\": \"#219ebc\", \"B\": \"#ffb703\"}\n",
    "sns.scatterplot(umap_df_standardized, x=\"UMAP1\", y=\"UMAP2\", \n",
    "                hue=\"diagnosis\", palette=color_mapping, s=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scot_env",
   "language": "python",
   "name": "scot_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
